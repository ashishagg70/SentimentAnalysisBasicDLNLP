{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "max_len=50\n",
    "NUMBER_CLASSES = 5\n",
    "EMBDIM=300\n",
    "hidden_layer_count=1\n",
    "hidden_layer_numunits=[256]\n",
    "embedding_file='../cc.en.300.vec'\n",
    "# embedding_file='../GoogleNews-vectors-negative300.bin'\n",
    "activation_name='relu'\n",
    "binary=True\n",
    "fastText=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = defaultdict(int)\n",
    "f= open('vocab.csv','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    words=line.split(',')\n",
    "    word_to_index[words[0]]=int(words[1].rstrip())\n",
    "\n",
    "# embeddings = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=defaultdict(lambda:np.zeros((1,EMBDIM)))\n",
    "f=open(embedding_file, 'r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "for line in lines[fastText:]:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], \"float32\")\n",
    "    embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return [i.lower() for i in text]\n",
    "\n",
    "def remove_punctuation(text):\n",
    "#     return [i.translate(str.maketrans(dict.fromkeys(string.punctuation))) for i in text]\n",
    "    return [i.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_data(text):\n",
    "    \n",
    "#     m=len(text)\n",
    "#     text_encoded = np.zeros((m,max_len))\n",
    "#     for i in range(m):\n",
    "#         sentence_words = text[i]\n",
    "#         j = 0\n",
    "#         for w in sentence_words:\n",
    "#             text_encoded[i, j] = word_to_index.get(w)\n",
    "#             j = j + 1\n",
    "\n",
    "            \n",
    "#     return text_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(text):\n",
    "#         print('idx',idx)\n",
    "    m=len(text)\n",
    "    text_encoded = np.zeros((m,max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = text[i]\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            text_encoded[i, j] = word_to_index.get(w)\n",
    "            j = j + 1\n",
    "    return text_encoded\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    newtext = []\n",
    "    f=open('stopwords.txt','r')\n",
    "    stopwordList=f.readlines()\n",
    "    stopwordList=[st.strip() for st in stopwordList]\n",
    "    f.close()\n",
    "#     stopwordList = (stopwords.words('english'))\n",
    "#     stopwordList=set(remove_punctuation(stopwordList))\n",
    "    #print('-----', stopwordList, '---------')\n",
    "    for tokens in text:\n",
    "        newtext.append([w for w in tokens if not w in stopwordList])\n",
    "    #print(newtext[0])\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16599"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tokenization(text):\n",
    "    return [word_tokenize(i) for i in text]\n",
    "\n",
    "def perform_padding(data):\n",
    "    pass\n",
    "#     return [list(np.pad(sent, (0, MAX_SENTENCE_LENGTH - len(sent)), 'constant', constant_values='0')) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, isTrain=True):\n",
    "    review = data[\"reviews\"]\n",
    "    review = convert_to_lower(review)\n",
    "    review = remove_punctuation(review)\n",
    "    review = perform_tokenization(review)\n",
    "    review = remove_stopwords(review)\n",
    "#     print(review[1])\n",
    "    review = encode_data(review)\n",
    "#     print(review[1])\n",
    "    global vocab_len\n",
    "    vocab_len=len(word_to_index)+1\n",
    "    \n",
    "    #review = perform_padding(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer():      \n",
    "    emb_dim = EMBDIM\n",
    "    global vocab_len\n",
    "    print(vocab_len)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "#     print(emb_matrix.shape)\n",
    "    for word, idx in word_to_index.items():\n",
    "        try:\n",
    "            emb_matrix[idx, :] = embeddings[word]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_len,output_dim=emb_dim,weights=[emb_matrix],input_length=max_len,trainable=True)\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(x):\n",
    "    expX = K.exp(x-K.reshape(K.max(x, axis=1), (K.shape(x)[0], 1)))\n",
    "    s = K.reshape(K.sum(expX, axis=1), (K.shape(x)[0], 1))\n",
    "    return expX / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self, reviews, ratings, val_reviews,val_ratings):\n",
    "        self.reviews = np.array(reviews, dtype='float32')\n",
    "        print('train_data_shape {}'.format(self.reviews.shape))\n",
    "        self.ratings = tf.keras.utils.to_categorical(y=ratings-1,num_classes=NUMBER_CLASSES)\n",
    "        val_ratings = tf.keras.utils.to_categorical(y=val_ratings-1,num_classes=NUMBER_CLASSES)\n",
    "        self.val_data=(val_reviews,val_ratings)\n",
    "        self.model = None\n",
    "\n",
    "    def build_nn(self):\n",
    "        \n",
    "        sentence_indices = tf.keras.layers.Input(shape=(max_len,),dtype='int32')\n",
    "        embedding = embedding_layer()\n",
    "        X = embedding(sentence_indices)\n",
    "        X = tf.keras.layers.Flatten()(X)\n",
    "#         X = tf.keras.layers.AveragePooling1D(pool_size=max_len)(X)\n",
    "#         X = tf.keras.layers.Flatten()(X)\n",
    "    \n",
    "        for i in range(hidden_layer_count):\n",
    "            X = tf.keras.layers.Dense(units=hidden_layer_numunits[i],kernel_initializer='glorot_uniform')(X)\n",
    "            X = tf.keras.layers.Dropout(rate=0.2)(X)\n",
    "            X=tf.keras.layers.Activation(activation=activation_name)(X)\n",
    "        \n",
    "        X = tf.keras.layers.Dense(units=5,activation='softmax')(X)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=sentence_indices,outputs=X)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "#         featureDim = self.reviews.shape[1]\n",
    "#         self.model = tf.keras.Sequential()\n",
    "#         self.model.add(tf.keras.layers.Dense(NUMBER_CLASSES, activation=softmax_activation, input_shape=(featureDim,)))\n",
    "#         self.model.summary()\n",
    "#         lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3,decay_steps=10000,decay_rate=0.9)\n",
    "#         self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "#                            metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    def train_nn(self, batch_size, epochs):\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1,patience=3)\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(self.scheduler)\n",
    "        self.model.fit(self.reviews,self.ratings, epochs=epochs, batch_size=batch_size,validation_data=self.val_data,callbacks=[es])\n",
    "#         self.model.fit(Generator(self.reviews,self.ratings,batch_size=batch_size), epochs=epochs, batch_size=batch_size,validation_data=self.val_data,callbacks=[es],steps_per_epoch=40)\n",
    "    def scheduler(self,epoch, lr):\n",
    "        if epoch < 5:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "    def predict(self, reviews):\n",
    "        reviews = np.array(reviews, dtype='float32')\n",
    "        return np.argmax(self.model.predict(reviews), axis=1) + 1\n",
    "    \n",
    "    def predictWithPr(self, reviews):\n",
    "        reviews = np.array(reviews, dtype='float32')\n",
    "        return self.model.predict(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    # Class is a dataset wrapper for better training performance\n",
    "    def __init__(self, x_set, y_set, batch_size=256):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        y_temp=np.argmax(y_set,axis=1)+1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        print(x_set.shape,y_set.shape)\n",
    "            \n",
    "    \n",
    "        self.c1_review = x_set[y_temp==1,:]\n",
    "        self.c2_review = x_set[y_temp==2,:]\n",
    "        self.c3_review = x_set[y_temp==3,:]\n",
    "        self.c4_review = x_set[y_temp==4,:]\n",
    "        self.c5_review = x_set[y_temp==5,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.len1=len(self.c1_review)\n",
    "        self.len2=len(self.c2_review)\n",
    "        self.len3=len(self.c3_review)\n",
    "        self.len4=len(self.c4_review)\n",
    "        self.len5=len(self.c5_review)\n",
    "\n",
    "        self.c1_ratings = y_set[y_temp==1,:]\n",
    "        self.c2_ratings = y_set[y_temp==2,:]\n",
    "        self.c3_ratings = y_set[y_temp==3,:]\n",
    "        self.c4_ratings = y_set[y_temp==4,:]\n",
    "        self.c5_ratings = y_set[y_temp==5,:]\n",
    "        self.batch_1=int(batch_size*0.2)\n",
    "        self.batch_2=int(batch_size*0.2)\n",
    "        self.batch_3=int(batch_size*0.2)\n",
    "        self.batch_4=int(batch_size*0.2)\n",
    "        self.batch_5=self.batch_size-self.batch_1-self.batch_2-self.batch_3-self.batch_4\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.x.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx1=np.random.choice(np.arange(self.len1),replace=False,size=self.batch_1)\n",
    "        idx2=np.random.choice(np.arange(self.len2),replace=False,size=self.batch_2)\n",
    "        idx3=np.random.choice(np.arange(self.len3),replace=False,size=self.batch_3)\n",
    "        idx4=np.random.choice(np.arange(self.len4),replace=False,size=self.batch_4)\n",
    "        idx5=np.random.choice(np.arange(self.len5),replace=False,size=self.batch_5)\n",
    "        \n",
    "        batch_x=[]\n",
    "        batch_x.extend(self.c1_review[idx1,:])\n",
    "        batch_x.extend(self.c2_review[idx2,:])\n",
    "        batch_x.extend(self.c3_review[idx3,:])\n",
    "        batch_x.extend(self.c4_review[idx4,:])\n",
    "        batch_x.extend(self.c5_review[idx5,:])\n",
    "        batch_x=np.array(batch_x)\n",
    "        \n",
    "        batch_y=[]\n",
    "        batch_y.extend(self.c1_ratings[idx1,:])\n",
    "        batch_y.extend(self.c2_ratings[idx2,:])\n",
    "        batch_y.extend(self.c3_ratings[idx3,:])\n",
    "        batch_y.extend(self.c4_ratings[idx4,:])\n",
    "        batch_y.extend(self.c5_ratings[idx5,:])\n",
    "        batch_y=np.array(batch_y)\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = np.array(train_df[\"ratings\"])\n",
    "train_reviews = np.array(preprocess_data(train_df))\n",
    "val_ratings = np.array(val_df[\"ratings\"])\n",
    "val_reviews = np.array(preprocess_data(val_df, False))\n",
    "test_reviews = preprocess_data(test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_ratings = np.array(train_data[\"ratings\"])\n",
    "# train_reviews = np.array(preprocess_data(train_data))\n",
    "# test_reviews = preprocess_data(test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10996. 10049.  1676.  4517. 16351.  8102. 10830. 11451.  6171.  2639.\n",
      "  6415.  9904.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shape (40000, 50)\n",
      "16600\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 50, 300)           4980000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               3840256   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 8,821,541\n",
      "Trainable params: 8,821,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet(train_reviews, train_ratings, val_reviews,val_ratings)\n",
    "model.build_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, epochs = 256, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 1.2086 - accuracy: 0.6118 - val_loss: 0.9205 - val_accuracy: 0.6725\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.8621 - accuracy: 0.6846 - val_loss: 0.7922 - val_accuracy: 0.7119\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.7292 - accuracy: 0.7328 - val_loss: 0.7413 - val_accuracy: 0.7245\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6598 - accuracy: 0.7576 - val_loss: 0.7153 - val_accuracy: 0.7343\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6044 - accuracy: 0.7749 - val_loss: 0.7021 - val_accuracy: 0.7406\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5650 - accuracy: 0.7920 - val_loss: 0.6955 - val_accuracy: 0.7454\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.5143 - accuracy: 0.8145 - val_loss: 0.6934 - val_accuracy: 0.7471\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.4754 - accuracy: 0.8340 - val_loss: 0.6928 - val_accuracy: 0.7458\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 0.4359 - accuracy: 0.8506 - val_loss: 0.7002 - val_accuracy: 0.7473\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.3943 - accuracy: 0.8676 - val_loss: 0.7095 - val_accuracy: 0.7476\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.3506 - accuracy: 0.8851 - val_loss: 0.7200 - val_accuracy: 0.7448\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.train_nn(batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Train data evaluation metrices==========================\n",
      "0.9087\n",
      "(0.9069289392005008, 0.9087, 0.9048030580895121, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"=================Train data evaluation metrices==========================\")\n",
    "# evaluation_matrices(train_ratings, model.predict(train_reviews))\n",
    "\n",
    "print(accuracy_score(train_ratings,model.predict(train_reviews)))\n",
    "print(precision_recall_fscore_support(train_ratings,model.predict(train_reviews),average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Test data evaluation metrices==========================\n",
      "0.6831\n",
      "(0.6461195182460066, 0.6831, 0.658885135673812, None)\n"
     ]
    }
   ],
   "source": [
    "print(\"=================Test data evaluation metrices==========================\")\n",
    "\n",
    "testPredictions = model.predict(test_reviews)\n",
    "test_ground_truth = np.array(pd.read_csv('gold_test.csv')['ratings'])\n",
    "\n",
    "# evaluation_matrices(test_ground_truth, testPredictions)\n",
    "print(accuracy_score(test_ground_truth, testPredictions))\n",
    "print(precision_recall_fscore_support(test_ground_truth, testPredictions,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 871,   73,  132,   34,  161],\n",
       "       [ 245,   72,  147,   53,  113],\n",
       "       [ 116,   60,  329,  180,  226],\n",
       "       [  40,   21,  199,  342,  802],\n",
       "       [  61,   19,  129,  358, 5217]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_ground_truth, testPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0889481e-05 6.6216812e-06 1.6103895e-05 1.0148172e-03 9.9895155e-01]\n",
      " [6.3093895e-01 1.1721336e-01 1.5240605e-01 5.3481828e-02 4.5959737e-02]\n",
      " [3.9870939e-03 2.7707808e-03 5.7733301e-03 4.2444628e-02 9.4502413e-01]\n",
      " [7.9690570e-01 1.4611633e-01 4.6300188e-02 8.3301505e-03 2.3475683e-03]\n",
      " [4.6778068e-01 3.1021202e-01 9.6583299e-02 1.0485898e-01 2.0564996e-02]\n",
      " [1.2224510e-01 1.8808168e-01 2.2975633e-01 3.7861428e-01 8.1302680e-02]\n",
      " [6.1102964e-02 4.4543631e-02 1.1672455e-01 3.6286518e-01 4.1476372e-01]\n",
      " [4.7890833e-01 1.9727629e-01 1.1896280e-01 1.1856668e-01 8.6285912e-02]\n",
      " [5.2285904e-01 9.0626404e-02 8.9382775e-02 2.2898847e-01 6.8143323e-02]\n",
      " [8.0277771e-01 1.4733925e-01 2.4208011e-02 1.5756914e-02 9.9181440e-03]]\n"
     ]
    }
   ],
   "source": [
    "ip_data = pd.read_csv(\"input.csv\")\n",
    "ip_reviews = preprocess_data(ip_data, False)\n",
    "pred = model.predictWithPr(ip_reviews)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Amazing!! I love and swear by this stuff. A mu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This product came in pieces .... would NOT rec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This is awesome product</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>does not work disappointed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Product is not bad but works.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Product is not good but works.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Product is good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>product is not good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Product is bad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Product is not bad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews  rating\n",
       "0           0  Amazing!! I love and swear by this stuff. A mu...       5\n",
       "1           1  This product came in pieces .... would NOT rec...       1\n",
       "2           2                            This is awesome product       5\n",
       "3           3                        does not work disappointed.       1\n",
       "4           4                      Product is not bad but works.       1\n",
       "5           5                     Product is not good but works.       4\n",
       "6           6                                    Product is good       5\n",
       "7           7                                product is not good       1\n",
       "8           8                                     Product is bad       1\n",
       "9           9                                 Product is not bad       1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_data[\"rating\"]=np.argmax(pred, axis=1) + 1\n",
    "ip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.model.save(filepath='./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    1       0.89      0.92      0.90      3229\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "without_hidden_layer(word2vec_300d)\n",
    "=================Train data evaluation metrices==========================\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.89      0.92      0.90      3229\n",
    "           2       0.81      0.94      0.87      1766\n",
    "           3       0.72      0.86      0.78      2915\n",
    "           4       0.76      0.55      0.64      5519\n",
    "           5       0.92      0.94      0.93     26571\n",
    "\n",
    "    accuracy                           0.88     40000\n",
    "   macro avg       0.82      0.84      0.83     40000\n",
    "weighted avg       0.88      0.88      0.87     40000\n",
    "\n",
    "=================Test data evaluation metrices==========================\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.63      0.64      0.64      1271\n",
    "           2       0.23      0.21      0.22       630\n",
    "           3       0.31      0.40      0.35       911\n",
    "           4       0.36      0.22      0.27      1404\n",
    "           5       0.83      0.88      0.86      5784\n",
    "\n",
    "    accuracy                           0.67     10000\n",
    "   macro avg       0.47      0.47      0.47     10000\n",
    "weighted avg       0.66      0.67      0.66     10000\n",
    "\n",
    "loss: 0.4769 - accuracy: 0.8615 - val_loss: 0.7254 - val_accuracy: 0.7344"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
